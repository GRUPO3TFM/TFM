{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/GRUPO3TFM/TFM/blob/main/WEATHER_WEBSCRAPPING_20ciudades.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Listado de Ciudades / Estaciones Weather**\n",
        "\n",
        "- Berlin  00433\n",
        "\n",
        "- Hamburg 01975\n",
        "\n",
        "- Munchen 03379\n",
        "\n",
        "- Koln 02667\n",
        "\n",
        "- Frankfurt am main 01420\n",
        "\n",
        "- Stuttgart 04928\n",
        "\n",
        "- Düsseldorf 01078\n",
        "\n",
        "- Leipzig 02928\n",
        "\n",
        "- Dortmund 06043\n",
        "\n",
        "- Essen 14146\n",
        "\n",
        "- Bremen 00691\n",
        "\n",
        "- Dresden 01048\n",
        "\n",
        "- Hannover 02014\n",
        "\n",
        "- Nürnberg 03668\n",
        "\n",
        "- Duisburg 13670\n",
        "\n",
        "- Bochum 00555\n",
        "\n",
        "- Wuppertal 05717\n",
        "\n",
        "- Bielefeld 07106\n",
        "\n",
        "- Bonn 00599\n",
        "\n",
        "- Münster(Westf) 01766"
      ],
      "metadata": {
        "id": "us5LFMwd9A3T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Temperatura Min y Maximo ( medicion cada 10min )**\n",
        "\n",
        "\n",
        "source:   https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/extreme_temperature/\n",
        ":\n",
        "Description Dataset:  DESCRIPTION_obsgermany_climate_10min_extreme_temperature_en.pdf (dwd.de)"
      ],
      "metadata": {
        "id": "zGZ36wdKKiyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Extreme Wind ( medicion cada hora)**  \n",
        "* Wind Speed and Wind Gust / Velocidad del viento y Ráfaga de viento )\n",
        "\n",
        "Ráfaga de viento ( aumento repentino y temporal en la velocidad del viento por encima de la velocidad promedio. ráfagas de viento pueden ocurrir en condiciones de viento variable, como tormentas o frentes de viento. La windgust se mide como la velocidad máxima alcanzada durante la ráfaga de viento y se expresa en las mismas unidades que la windspeed (km/h o m/s).\\\n",
        "\n",
        "Source: Ihttps://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/extreme_wind/\n",
        "\n",
        "Description Dataset: DESCRIPTION_obsgermany_climate_hourly_extreme_wind_en.pdf (dwd.de)\n"
      ],
      "metadata": {
        "id": "Utml4bmFJlr1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Precipitation ( medicion cada hora )**\n",
        "\n",
        "Source: Index of /climate_environment/CDC/observations_germany/climate/hourly/precipitation/ (dwd.de)\\\n",
        "Description Dataset: DESCRIPTION_obsgermany_climate_hourly_precipitation_en.pdf (dwd.de)\n"
      ],
      "metadata": {
        "id": "uyYY6RaCJ77k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Weather Phenomena (  medicion cada Hora )**\n",
        "\n",
        "varios registros fenómenos climáticos, como nieve, precipitación y nieve, etc\\\n",
        "Source: Index of: https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/extreme_temperature/recent/\n"
      ],
      "metadata": {
        "id": "GrB7OO83KVWj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install requests beautifulsoup4\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fxOcD5uV3J3C",
        "outputId": "1b39ed90-1e4b-480a-c33b-eb73b51bcd20"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.31.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.10/dist-packages (4.12.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.6.2)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4) (2.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "jlT2fioSKPhR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#cargamos llibrerias necesarias\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import urllib.request\n",
        "\n",
        "import zipfile\n",
        "import os\n",
        "from google.colab import files"
      ],
      "metadata": {
        "id": "b9QvGMRxK9ns"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** TEMPERATURAS MIN Y MAX**"
      ],
      "metadata": {
        "id": "K456w0ntXlwe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######                              T E M P E R A T U R A     M I N    Y   M A X I M O\n",
        "\n",
        "\n",
        "#URL de la página web\n",
        "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/10_minutes/extreme_temperature/recent/\"\n",
        "\n",
        "#Mapeo de IDs de estaciones a nombres de ciudades\n",
        "station_to_city = {\n",
        "    '00433': 'Berlin',\n",
        "    '01975': 'Hamburg',\n",
        "    '03379': 'Munchen',\n",
        "    '02667': 'Bonn',\n",
        "    '01420': 'Frankfurt_am_Main',\n",
        "    '04928': 'Stuttgart',\n",
        "    '01078': 'Düsseldorf',\n",
        "    '02928': 'Leipzig',\n",
        "    '01303': 'Essen',\n",
        "    '00691': 'Bremen',\n",
        "    '01048': 'Dresden',\n",
        "    '02014': 'Hannover',\n",
        "    '03668': 'Nürnberg',\n",
        "    '13670': 'Duisburg',\n",
        "    '00555': 'Bochum',\n",
        "    '05717': 'Wuppertal',\n",
        "    '07106': 'Bielefeld',\n",
        "    '02968': 'Koln',\n",
        "    '01766': 'Münster(Westf)'\n",
        "}\n",
        "\n",
        "# IDs de las estaciones a buscar\n",
        "station_ids = list(station_to_city.keys())\n",
        "\n",
        "# Realizar una solicitud GET a la página web\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsear el contenido HTML de la página web\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontrar todos los enlaces en la página\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "        # Crear un set para rastrear archivos descargados\n",
        "    downloaded_files = set()\n",
        "\n",
        "    # Iterar sobre cada ID de estación\n",
        "    for station_id in station_ids:\n",
        "        # Buscar el enlace que contiene el ID de la estación\n",
        "        for link in links:\n",
        "            if f'10minutenwerte_extrema_temp_{station_id}_akt.zip' in link.get('href'):\n",
        "                file_url = url + link.get('href')\n",
        "                print(f\"Descargando el archivo desde {file_url}\")\n",
        "\n",
        "        # Descargar el archivo y guardarlo en la cloud\n",
        "                file_name = link.get('href')\n",
        "                urllib.request.urlretrieve(file_url, file_name)\n",
        "                print(f\"Archivo {file_name} descargado exitosamente.\")\n",
        "\n",
        "                # Extraer el archivo zip\n",
        "                with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(f\"/content/{station_id}\")\n",
        "\n",
        "                # Renombrar y descargar archivos extraídos a la máquina local\n",
        "                extracted_files = os.listdir(f\"/content/{station_id}\")\n",
        "                for extracted_file in extracted_files:\n",
        "                    original_file_path = f\"/content/{station_id}/{extracted_file}\"\n",
        "                    city_name = station_to_city[station_id]\n",
        "                    new_file_name = f\"temperature_{station_id}_{city_name}.txt\"  # Adjust the extension if necessary\n",
        "                    new_file_path = f\"/content/{station_id}/{new_file_name}\"\n",
        "\n",
        "                    # Renombrar archivo\n",
        "                    os.rename(original_file_path, new_file_path)\n",
        "\n",
        "                    # Chequear si el archivo ya fue descargado\n",
        "                    if new_file_name not in downloaded_files:\n",
        "                        files.download(new_file_path)\n",
        "                        downloaded_files.add(new_file_name)\n",
        "                break\n",
        "else:\n",
        "    print(\"No se pudo acceder a la página web.\")\n"
      ],
      "metadata": {
        "id": "OfVKZxnyjz5x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RAIN - PRECIPITACION**"
      ],
      "metadata": {
        "id": "t7ei8ESzXuEg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######                                          R A I N         -     P R E C I P I T A C I O N\n",
        "\n",
        "# URL de la página web\n",
        "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/precipitation/recent/\"\n",
        "\n",
        "# Mapeo de IDs de estaciones a nombres de ciudades\n",
        "station_to_city = {\n",
        "    '00433': 'Berlin',\n",
        "    '01975': 'Hamburg',\n",
        "    '03379': 'Munchen',\n",
        "    '02667': 'Bonn',\n",
        "    '01420': 'Frankfurt_am_Main',\n",
        "    '04928': 'Stuttgart',\n",
        "    '01078': 'Düsseldorf',\n",
        "    '02928': 'Leipzig',\n",
        "    '01303': 'Essen',\n",
        "    '00691': 'Bremen',\n",
        "    '01048': 'Dresden',\n",
        "    '02014': 'Hannover',\n",
        "    '03668': 'Nürnberg',\n",
        "    '13670': 'Duisburg',\n",
        "    '00555': 'Bochum',\n",
        "    '05717': 'Wuppertal',\n",
        "    '07106': 'Bielefeld',\n",
        "    '02968': 'Koln',\n",
        "    '01766': 'Münster(Westf)'\n",
        "}\n",
        "\n",
        "# IDs de las estaciones a buscar\n",
        "station_ids = list(station_to_city.keys())\n",
        "\n",
        "# Crear directorio para guardar los archivos si no existe\n",
        "upload_dir = \"/content/lluvia\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "# Realizar una solicitud GET a la página web\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsear el contenido HTML de la página web\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontrar todos los enlaces en la página\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    # Crear un set para rastrear archivos descargados\n",
        "    downloaded_files = set()\n",
        "\n",
        "    # Iterar sobre cada ID de estación\n",
        "    for station_id in station_ids:\n",
        "        # Buscar el enlace que contiene el ID de la estación\n",
        "        for link in links:\n",
        "            if f'stundenwerte_RR_{station_id}_akt.zip' in link.get('href'):\n",
        "                file_url = url + link.get('href')\n",
        "                print(f\"Descargando el archivo desde {file_url}\")\n",
        "\n",
        "                # Descargar el archivo y guardarlo\n",
        "                file_name = link.get('href').split('/')[-1]\n",
        "                local_zip_path = os.path.join(upload_dir, file_name)\n",
        "                urllib.request.urlretrieve(file_url, local_zip_path)\n",
        "                print(f\"Archivo {file_name} descargado exitosamente.\")\n",
        "\n",
        "                # Extraer el archivo zip\n",
        "                with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(upload_dir)\n",
        "\n",
        "                # Renombrar y descargar el archivo relevante\n",
        "                extracted_files = os.listdir(upload_dir)\n",
        "                for extracted_file in extracted_files:\n",
        "                    if extracted_file.startswith(\"produkt_rr_stunde\") and extracted_file.endswith(\".txt\"):\n",
        "                        original_file_path = os.path.join(upload_dir, extracted_file)\n",
        "                        city_name = station_to_city[station_id]\n",
        "                        new_file_name = f\"Precipitacion_{city_name}_{station_id}.txt\"\n",
        "                        new_file_path = os.path.join(upload_dir, new_file_name)\n",
        "\n",
        "                        # Renombrar archivo\n",
        "                        os.rename(original_file_path, new_file_path)\n",
        "\n",
        "                        # Chequear si el archivo ya fue descargado\n",
        "                        if new_file_name not in downloaded_files:\n",
        "                            files.download(new_file_path)\n",
        "                            downloaded_files.add(new_file_name)\n",
        "                break\n",
        "else:\n",
        "    print(\"No se pudo acceder a la página web.\")\n",
        "\n",
        "# Verificar los archivos en la carpeta después de renombrar\n",
        "print(\"\\nArchivos después de renombrar:\")\n",
        "print(os.listdir(upload_dir))\n"
      ],
      "metadata": {
        "id": "pOqnk-WftxQM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "** EXTREME WIND - VIENTO EXTREMO**"
      ],
      "metadata": {
        "id": "orT_Iz_LX78m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#######                                          W I N D          -        V I E N T O    E X T R E M O\n",
        "\n",
        "# URL de la página web\n",
        "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/hourly/extreme_wind/recent//\"\n",
        "\n",
        "# Mapeo de IDs de estaciones a nombres de ciudades\n",
        "station_to_city = {\n",
        "    '00433': 'Berlin',\n",
        "    '01975': 'Hamburg',\n",
        "    '03379': 'Munchen',\n",
        "    '02667': 'Bonn',\n",
        "    '01420': 'Frankfurt_am_Main',\n",
        "    '04928': 'Stuttgart',\n",
        "    '01078': 'Düsseldorf',\n",
        "    '02928': 'Leipzig',\n",
        "    '01303': 'Essen',\n",
        "    '00691': 'Bremen',\n",
        "    '01048': 'Dresden',\n",
        "    '02014': 'Hannover',\n",
        "    '03668': 'Nürnberg',\n",
        "    '13670': 'Duisburg',\n",
        "    '00555': 'Bochum',\n",
        "    '05717': 'Wuppertal',\n",
        "    '07106': 'Bielefeld',\n",
        "    '02968': 'Koln',\n",
        "    '01766': 'Münster(Westf)'\n",
        "}\n",
        "\n",
        "# IDs de las estaciones a buscar\n",
        "station_ids = list(station_to_city.keys())\n",
        "\n",
        "# Crear directorio para guardar los archivos si no existe\n",
        "upload_dir = \"/content/viento\"\n",
        "os.makedirs(upload_dir, exist_ok=True)\n",
        "\n",
        "# Realizar una solicitud GET a la página web\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsear el contenido HTML de la página web\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontrar todos los enlaces en la página\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    # Crear un set para rastrear archivos descargados\n",
        "    downloaded_files = set()\n",
        "\n",
        "    # Iterar sobre cada ID de estación\n",
        "    for station_id in station_ids:\n",
        "        # Buscar el enlace que contiene el ID de la estación\n",
        "        for link in links:\n",
        "            if f'stundenwerte_FX_{station_id}_akt.zip' in link.get('href'):\n",
        "                file_url = url + link.get('href')\n",
        "                print(f\"Descargando el archivo desde {file_url}\")\n",
        "\n",
        "                # Descargar el archivo y guardarlo\n",
        "                file_name = link.get('href').split('/')[-1]\n",
        "                local_zip_path = os.path.join(upload_dir, file_name)\n",
        "                urllib.request.urlretrieve(file_url, local_zip_path)\n",
        "                print(f\"Archivo {file_name} descargado exitosamente.\")\n",
        "\n",
        "                # Extraer el archivo zip\n",
        "                with zipfile.ZipFile(local_zip_path, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(upload_dir)\n",
        "\n",
        "                # Renombrar y descargar el archivo relevante\n",
        "                extracted_files = os.listdir(upload_dir)\n",
        "                for extracted_file in extracted_files:\n",
        "                    if extracted_file.startswith(\"produkt_fx_stunde\") and extracted_file.endswith(\".txt\"):\n",
        "                        original_file_path = os.path.join(upload_dir, extracted_file)\n",
        "                        city_name = station_to_city[station_id]\n",
        "                        new_file_name = f\"Extreme_wind_{city_name}_{station_id}.txt\"\n",
        "                        new_file_path = os.path.join(upload_dir, new_file_name)\n",
        "\n",
        "                        # Renombrar archivo\n",
        "                        os.rename(original_file_path, new_file_path)\n",
        "\n",
        "                        # Chequear si el archivo ya fue descargado\n",
        "                        if new_file_name not in downloaded_files:\n",
        "                            files.download(new_file_path)\n",
        "                            downloaded_files.add(new_file_name)\n",
        "                break\n",
        "else:\n",
        "    print(\"No se pudo acceder a la página web.\")\n",
        "\n",
        "# Verificar los archivos en la carpeta después de renombrar\n",
        "print(\"\\nArchivos después de renombrar:\")\n",
        "print(os.listdir(upload_dir))"
      ],
      "metadata": {
        "id": "yx0xDqbSkmxv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "no hemos usado al final:"
      ],
      "metadata": {
        "id": "rPDXXhimYLPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "######          W E A T H E R    P H E N O M E N A\n",
        "\n",
        "# URL de la página web\n",
        "url = \"https://opendata.dwd.de/climate_environment/CDC/observations_germany/climate/daily/weather_phenomena/recent/\"\n",
        "\n",
        "## Mapeo de IDs de estaciones a nombres de ciudades\n",
        "station_to_city = {\n",
        "    '00433': 'Berlin',\n",
        "    '01975': 'Hamburg',\n",
        "    '03379': 'Munchen',\n",
        "    '02667': 'Koln',\n",
        "    '01420': 'Frankfurt_am_main',\n",
        "    '04928': 'Stuttgart',\n",
        "    '01078': 'Düsseldorf',\n",
        "    '02928': 'Leipzig',\n",
        "    '06043': 'Dortmund',\n",
        "    '14146': 'Essen',\n",
        "    '00691': 'Bremen',\n",
        "    '01048': 'Dresden',\n",
        "    '02014': 'Hannover',\n",
        "    '03668': 'Nürnberg',\n",
        "    '13670': 'Duisburg',\n",
        "    '00555': 'Bochum',\n",
        "    '05717': 'Wuppertal',\n",
        "    '07106': 'Bielefeld',\n",
        "    '00599': 'Bonn',\n",
        "    '01766': 'Münster(Westf)'\n",
        "}\n",
        "\n",
        "# IDs de las estaciones a buscar\n",
        "station_ids = list(station_to_city.keys())\n",
        "\n",
        "# Realizar una solicitud GET a la página web\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Parsear el contenido HTML de la página web\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Encontrar todos los enlaces en la página\n",
        "    links = soup.find_all('a')\n",
        "\n",
        "    # Iterar sobre cada ID de estación\n",
        "    for station_id in station_ids:\n",
        "        # Buscar el enlace que contiene el ID de la estación\n",
        "        for link in links:\n",
        "            if f'wetter_tageswerte_{station_id}_akt.zip' in link.get('href'):\n",
        "                file_url = url + link.get('href')\n",
        "                print(f\"Descargando el archivo desde {file_url}\")\n",
        "\n",
        "\n",
        "   # Descargar el archivo y guardarlo en la cloud\n",
        "                file_name = link.get('href')\n",
        "                urllib.request.urlretrieve(file_url, file_name)\n",
        "                print(f\"Archivo {file_name} descargado exitosamente.\")\n",
        "\n",
        "                # Extraer el archivo zip\n",
        "                with zipfile.ZipFile(file_name, 'r') as zip_ref:\n",
        "                    zip_ref.extractall(f\"/content/{station_id}\")\n",
        "\n",
        "                # Renombrar y descargar archivos extraídos a la máquina local\n",
        "                extracted_files = os.listdir(f\"/content/{station_id}\")\n",
        "                for extracted_file in extracted_files:\n",
        "                    original_file_path = f\"/content/{station_id}/{extracted_file}\"\n",
        "                    city_name = station_to_city[station_id]\n",
        "                    new_file_name = f\"weather_phenomena_{station_id}_{city_name}.txt\"\n",
        "                    new_file_path = f\"/content/{station_id}/{new_file_name}\"\n",
        "\n",
        "                    # Renombrar archivo\n",
        "                    os.rename(original_file_path, new_file_path)\n",
        "\n",
        "                    # Chequear si el archivo ya fue descargado\n",
        "                    if new_file_name not in downloaded_files:\n",
        "                        files.download(new_file_path)\n",
        "                        downloaded_files.add(new_file_name)\n",
        "                break\n",
        "else:\n",
        "    print(\"No se pudo acceder a la página web.\")\n"
      ],
      "metadata": {
        "id": "mc7yOcAxnlEq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}